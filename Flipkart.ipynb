{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--window-size=1920x1080\")\n",
    "chrome_driver = \"I:\\Web Scraping\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=chrome_driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# name,rating,price,links,count=[],[],[],[],[]\n",
    "# for p in range(1,2):    \n",
    "#     print(p)\n",
    "#     driver.get(f'https://www.flipkart.com/toys/puzzles-board-games/card-games/pr?sid=mgl%2Cqet&p%5B%5D=facets.rating%255B%255D%3D2%25E2%2598%2585%2B%2526%2Babove')\n",
    "#     data=driver.execute_script('return document.documentElement.outerHTML')\n",
    "#     soup=BeautifulSoup(data,'html.parser')\n",
    "#     for value in soup.findAll('div',{'class':'_1HmYoV _35HD7C'}):\n",
    "#         for val in value.findAll('div',{'class':'_3liAhj'}):\n",
    "#             def scrp_name():\n",
    "#                 naming='Null'\n",
    "#                 for tx in val.findAll('a',{'class':'_2cLu-l'}):\n",
    "#                     naming+=tx.text\n",
    "#                 return naming\n",
    "#             name.append(scrp_name())\n",
    "#         for val in value.findAll('div',{'class':'_3liAhj'}):\n",
    "#             def scrap():\n",
    "#                 count=''\n",
    "#                 rating='Null'\n",
    "#                 for rt in val.findAll('div',{'class':'hGSR34'}):\n",
    "#                     rating=rt.text\n",
    "#                 for ct in val.findAll('span',{'class':'_38sUEc'}):\n",
    "#                     count+=ct.text\n",
    "#                 return rating,count\n",
    "#             rating.append(scrap()[0])\n",
    "#             count.append(scrap()[1])\n",
    "#         for val in value.findAll('a',{'class':'_1Vfi6u'}):\n",
    "#             def scrap_price():\n",
    "#                 pricess='Null'\n",
    "#                 for vl in val.findAll('div',{'class':'_1vC4OE'}):\n",
    "#                     pricess+=vl.text\n",
    "#                 return pricess\n",
    "#             price.append(scrap_price())\n",
    "#         for val in value.findAll('div',{'class':'_3liAhj'}):\n",
    "#             for tx in val.findAll('a',{'class':'_2cLu-l'}):\n",
    "#                 links.append(\"www.flipkart.com\"+tx.get('href'))\n",
    "#     casual4=pd.DataFrame({'Name':name,'Price':price,'Links':links,'Rating':rating,'Count':count})\n",
    "#     casual4.to_csv(r'E:\\casual4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLIKPART REVIEWS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "links=[]\n",
    "# limit=0\n",
    "# driver.get(f'https://www.flipkart.com/camlin-brush-pen-12-shades/product-reviews/itmfc5bhzjfjvg8a?pid=ARTFCFXHFKVRXH4V&lid=LSTARTFCFXHFKVRXH4VGQ8MNY&marketplace=FLIPKART&page=1')\n",
    "# data=driver.execute_script('return document.documentElement.outerHTML')\n",
    "# soup=BeautifulSoup(data,'html.parser')\n",
    "# for val in soup.findAll('div',{'class':'_2zg3yZ _3KSYCY'}):\n",
    "#     for num in val.find('span'):\n",
    "#         limit=num[-1]\n",
    "for p in range(1,26):    \n",
    "    print(p)\n",
    "    driver.get(f'https://www.flipkart.com/mens-footwear/casual-shoes/sneakers~type/pr?sid=osp%2Ccil%2Ce1f&p%5B%5D=facets.rating%255B%255D%3D2%25E2%2598%2585%2B%2526%2Babove&page={p}')\n",
    "    data=driver.execute_script('return document.documentElement.outerHTML')\n",
    "    soup=BeautifulSoup(data,'html.parser')\n",
    "    for val in soup.findAll('div',{'class':'_1HmYoV _35HD7C'}):\n",
    "        for rvs in val.findAll('div',{'class':'IIdQZO _1SSAGr'}):\n",
    "            def scrap_price():\n",
    "                    rev='https://www.flipkart.com'\n",
    "                    for lnk in rvs.findAll('a',{'class':'_2mylT6'}):\n",
    "                            rev+=lnk.get('href')\n",
    "                    return rev\n",
    "            links.append(scrap_price())\n",
    "    sneaker_links2=pd.DataFrame({'Links':links})\n",
    "    sneaker_links2.to_csv(r'E:\\sneaker_links2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "links=[]\n",
    "# limit=0\n",
    "# driver.get(f'https://www.flipkart.com/camlin-brush-pen-12-shades/product-reviews/itmfc5bhzjfjvg8a?pid=ARTFCFXHFKVRXH4V&lid=LSTARTFCFXHFKVRXH4VGQ8MNY&marketplace=FLIPKART&page=1')\n",
    "# data=driver.execute_script('return document.documentElement.outerHTML')\n",
    "# soup=BeautifulSoup(data,'html.parser')\n",
    "# for val in soup.findAll('div',{'class':'_2zg3yZ _3KSYCY'}):\n",
    "#     for num in val.find('span'):\n",
    "#         limit=num[-1]\n",
    "for p in range(1,26):    \n",
    "    print(p)\n",
    "    driver.get(f'https://www.flipkart.com/mens-footwear/sandals-floaters/pr?sid=osp%2Ccil%2Ce83&p%5B%5D=facets.rating%255B%255D%3D1%25E2%2598%2585%2B%2526%2Babove&page={p}')\n",
    "    data=driver.execute_script('return document.documentElement.outerHTML')\n",
    "    soup=BeautifulSoup(data,'html.parser')\n",
    "    for val in soup.findAll('div',{'class':'_1HmYoV _35HD7C'}):\n",
    "        for rvs in val.findAll('div',{'class':'IIdQZO _1SSAGr'}):\n",
    "            def scrap_price():\n",
    "                    rev='https://www.flipkart.com'\n",
    "                    for lnk in rvs.findAll('a',{'class':'_2mylT6'}):\n",
    "                            rev+=lnk.get('href')\n",
    "                    return rev\n",
    "            links.append(scrap_price())\n",
    "    sandals_links1=pd.DataFrame({'Links':links})\n",
    "    sandals_links1.to_csv(r'D:\\sandals_links1.csv')\n",
    "    \n",
    "    \n",
    "for p in range(1,26):    \n",
    "    print(p)\n",
    "    driver.get(f'https://www.flipkart.com/mens-footwear/sandals-floaters/pr?sid=osp%2Ccil%2Ce83&p%5B%5D=facets.rating%255B%255D%3D2%25E2%2598%2585%2B%2526%2Babove&page={p}')\n",
    "    data=driver.execute_script('return document.documentElement.outerHTML')\n",
    "    soup=BeautifulSoup(data,'html.parser')\n",
    "    for val in soup.findAll('div',{'class':'_1HmYoV _35HD7C'}):\n",
    "        for rvs in val.findAll('div',{'class':'IIdQZO _1SSAGr'}):\n",
    "            def scrap_price():\n",
    "                    rev='https://www.flipkart.com'\n",
    "                    for lnk in rvs.findAll('a',{'class':'_2mylT6'}):\n",
    "                            rev+=lnk.get('href')\n",
    "                    return rev\n",
    "            links.append(scrap_price())\n",
    "    sandals_links2=pd.DataFrame({'Links':links})\n",
    "    sandals_links2.to_csv(r'D:\\sandals_links2.csv')\n",
    "    \n",
    "    \n",
    "for p in range(1,26):    \n",
    "    print(p)\n",
    "    driver.get(f'https://www.flipkart.com/mens-footwear/sandals-floaters/pr?sid=osp%2Ccil%2Ce83&p%5B%5D=facets.rating%255B%255D%3D3%25E2%2598%2585%2B%2526%2Babove&page={p}')\n",
    "    data=driver.execute_script('return document.documentElement.outerHTML')\n",
    "    soup=BeautifulSoup(data,'html.parser')\n",
    "    for val in soup.findAll('div',{'class':'_1HmYoV _35HD7C'}):\n",
    "        for rvs in val.findAll('div',{'class':'IIdQZO _1SSAGr'}):\n",
    "            def scrap_price():\n",
    "                    rev='https://www.flipkart.com'\n",
    "                    for lnk in rvs.findAll('a',{'class':'_2mylT6'}):\n",
    "                            rev+=lnk.get('href')\n",
    "                    return rev\n",
    "            links.append(scrap_price())\n",
    "    sandals_links3=pd.DataFrame({'Links':links})\n",
    "    sandals_links3.to_csv(r'D:\\sandals_links3.csv')\n",
    "    \n",
    "    \n",
    "for p in range(1,26):    \n",
    "    print(p)\n",
    "    driver.get(f'https://www.flipkart.com/mens-footwear/sandals-floaters/pr?sid=osp%2Ccil%2Ce83&p%5B%5D=facets.rating%255B%255D%3D4%25E2%2598%2585%2B%2526%2Babove&page={p}')\n",
    "    data=driver.execute_script('return document.documentElement.outerHTML')\n",
    "    soup=BeautifulSoup(data,'html.parser')\n",
    "    for val in soup.findAll('div',{'class':'_1HmYoV _35HD7C'}):\n",
    "        for rvs in val.findAll('div',{'class':'IIdQZO _1SSAGr'}):\n",
    "            def scrap_price():\n",
    "                    rev='https://www.flipkart.com'\n",
    "                    for lnk in rvs.findAll('a',{'class':'_2mylT6'}):\n",
    "                            rev+=lnk.get('href')\n",
    "                    return rev\n",
    "            links.append(scrap_price())\n",
    "    sandals_links4=pd.DataFrame({'Links':links})\n",
    "    sandals_links4.to_csv(r'D:\\sandals_links4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flikpart Shoes Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1=pd.read_csv(r'E:\\sneaker_links1.csv')\n",
    "df2=pd.read_csv(r'E:\\sneaker_links2.csv')\n",
    "df3=pd.read_csv(r'E:\\sneaker_links3.csv')\n",
    "df4=pd.read_csv(r'E:\\sneaker_links4.csv')\n",
    "\n",
    "\n",
    "# name,rating,price,links,count,temp=[],[],[],[],[],[]\n",
    "# for p in range(606,df1.shape[0]):    \n",
    "#     print(p)\n",
    "#     driver.get(df1.Links[p])\n",
    "#     data=driver.execute_script('return document.documentElement.outerHTML')\n",
    "#     soup=BeautifulSoup(data,'html.parser')\n",
    "#     for value in soup.findAll('div',{'class':'ooJZfD _2oZ8XT col-8-12'}):\n",
    "#         def scrp_name():\n",
    "#             naming='Null'\n",
    "#             for val in value.findAll('div',{'class':'_29OxBi'}):\n",
    "            \n",
    "#                 for tx in val.findAll('span',{'class':'_35KyD6'}):\n",
    "#                     naming+=tx.text\n",
    "#             return naming\n",
    "#         name.append(scrp_name())\n",
    "#         def scrap():\n",
    "#             count='Null'\n",
    "#             rating='Null'\n",
    "#             for val in value.findAll('div',{'class':'a7AaCC'}):           \n",
    "#                 for rt in val.findAll('div',{'class':'hGSR34 bqXGTW'}):\n",
    "#                     rating=rt.text\n",
    "#                 for ct in val.findAll('span',{'class':'_38sUEc'}):\n",
    "#                     count+=ct.text\n",
    "#             return rating,count\n",
    "#         rating.append(scrap()[0])\n",
    "#         count.append(scrap()[1])\n",
    "#         def scrap_price():\n",
    "#             pricess='Null'\n",
    "#             for val in value.findAll('div',{'class':'_3iZgFn _9Z7kX3'}):\n",
    "#                 for vl in val.findAll('div',{'class':'_1vC4OE _3qQ9m1'}):\n",
    "#                     pricess+=vl.text\n",
    "#             return pricess\n",
    "#         price.append(scrap_price())\n",
    "        \n",
    "#         def ss():\n",
    "#             mist='https://www.flipkart.com'\n",
    "#             for val in value.findAll('div',{'class':'col _39LH-M _3cycCZ'}):       \n",
    "#                 for tx in val.findAll('a'):\n",
    "#                     mist+=tx.get('href')\n",
    "#                     temp.append(mist)\n",
    "#                     mist='https://www.flipkart.com'\n",
    "#             return mist,temp\n",
    "#         a,b=ss()\n",
    "#         if b!=[]:\n",
    "#                 links.append(temp[-1])\n",
    "#         else:\n",
    "#                 links.append(a)\n",
    "#         temp=[]\n",
    "#     sneaker1_1=pd.DataFrame({'Name':name,'Price':price,'Links':links,'Rating':rating,'Count':count})\n",
    "#     sneaker1_1.to_csv(r'E:\\sneaker1_1.csv')\n",
    "\n",
    "\n",
    "# name,rating,price,links,count,temp=[],[],[],[],[],[]\n",
    "# for p in range(0,360):    \n",
    "#     print(p)\n",
    "#     driver.get(df2.Links[p])\n",
    "#     data=driver.execute_script('return document.documentElement.outerHTML')\n",
    "#     soup=BeautifulSoup(data,'html.parser')\n",
    "#     for value in soup.findAll('div',{'class':'ooJZfD _2oZ8XT col-8-12'}):\n",
    "#         def scrp_name():\n",
    "#             naming='Null'\n",
    "#             for val in value.findAll('div',{'class':'_29OxBi'}):\n",
    "            \n",
    "#                 for tx in val.findAll('span',{'class':'_35KyD6'}):\n",
    "#                     naming+=tx.text\n",
    "#             return naming\n",
    "#         name.append(scrp_name())      \n",
    "#         def scrap():\n",
    "#             count=''\n",
    "#             rating='Null'\n",
    "#             for val in value.findAll('div',{'class':'a7AaCC'}):\n",
    "#                 for rt in val.findAll('div',{'class':'hGSR34 bqXGTW'}):\n",
    "#                     rating=rt.text\n",
    "#                 for ct in val.findAll('span',{'class':'_38sUEc'}):\n",
    "#                     count+=ct.text\n",
    "#             return rating,count\n",
    "#         rating.append(scrap()[0])\n",
    "#         count.append(scrap()[1])\n",
    "#         def scrap_price():\n",
    "#             pricess='Null'\n",
    "#             for val in value.findAll('div',{'class':'_3iZgFn _9Z7kX3'}):\n",
    "#                 for vl in val.findAll('div',{'class':'_1vC4OE _3qQ9m1'}):\n",
    "#                     pricess+=vl.text\n",
    "#             return pricess\n",
    "#         price.append(scrap_price())\n",
    "#         def ss():\n",
    "#             mist='https://www.flipkart.com'\n",
    "#             for val in value.findAll('div',{'class':'col _39LH-M _3cycCZ'}):       \n",
    "#                 for tx in val.findAll('a'):\n",
    "#                     mist+=tx.get('href')\n",
    "#                     temp.append(mist)\n",
    "#                     mist='https://www.flipkart.com'\n",
    "#             return mist,temp\n",
    "#         a,b=ss()\n",
    "#         if b!=[]:\n",
    "#                 links.append(temp[-1])\n",
    "#         else:\n",
    "#                 links.append(a)\n",
    "#         temp=[]\n",
    "#     sneaker2_1=pd.DataFrame({'Name':name,'Price':price,'Links':links,'Rating':rating,'Count':count})\n",
    "#     sneaker2_1.to_csv(r'E:\\sneaker2_3.csv')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "name,rating,price,links,count,temp=[],[],[],[],[],[]\n",
    "for p in range(699,df3.shape[0]):    \n",
    "    print(p)\n",
    "    driver.get(df3.Links[p])\n",
    "    data=driver.execute_script('return document.documentElement.outerHTML')\n",
    "    soup=BeautifulSoup(data,'html.parser')\n",
    "    for value in soup.findAll('div',{'class':'ooJZfD _2oZ8XT col-8-12'}):\n",
    "        def scrp_name():\n",
    "            naming='Null'\n",
    "            for val in value.findAll('div',{'class':'_29OxBi'}):\n",
    "            \n",
    "                for tx in val.findAll('span',{'class':'_35KyD6'}):\n",
    "                    naming+=tx.text\n",
    "            return naming\n",
    "        name.append(scrp_name())\n",
    "        def scrap():\n",
    "            count=''\n",
    "            rating='Null'\n",
    "            for val in value.findAll('div',{'class':'a7AaCC'}):\n",
    "                for rt in val.findAll('div',{'class':'hGSR34 bqXGTW'}):\n",
    "                    rating=rt.text\n",
    "                for ct in val.findAll('span',{'class':'_38sUEc'}):\n",
    "                    count+=ct.text\n",
    "            return rating,count\n",
    "        rating.append(scrap()[0])\n",
    "        count.append(scrap()[1])\n",
    "        def scrap_price():\n",
    "            pricess='Null'\n",
    "            for val in value.findAll('div',{'class':'_3iZgFn _9Z7kX3'}):\n",
    "                for vl in val.findAll('div',{'class':'_1vC4OE _3qQ9m1'}):\n",
    "                    pricess+=vl.text\n",
    "            return pricess\n",
    "        price.append(scrap_price())\n",
    "        def ss():\n",
    "            mist='https://www.flipkart.com'\n",
    "            for val in value.findAll('div',{'class':'col _39LH-M _3cycCZ'}):       \n",
    "                for tx in val.findAll('a'):\n",
    "                    mist+=tx.get('href')\n",
    "                    temp.append(mist)\n",
    "                    mist='https://www.flipkart.com'\n",
    "            return mist,temp\n",
    "        a,b=ss()\n",
    "        if b!=[]:\n",
    "                links.append(temp[-1])\n",
    "        else:\n",
    "                links.append(a)\n",
    "        temp=[]\n",
    "    sneaker3_2=pd.DataFrame({'Name':name,'Price':price,'Links':links,'Rating':rating,'Count':count})\n",
    "    sneaker3_2.to_csv(r'E:\\sneaker3_2.csv')\n",
    "    \n",
    "    \n",
    "name,rating,price,links,count,temp=[],[],[],[],[],[]\n",
    "for p in range(0,df4.shape[0]):    \n",
    "    print(p)\n",
    "    driver.get(df4.Links[p])\n",
    "    data=driver.execute_script('return document.documentElement.outerHTML')\n",
    "    soup=BeautifulSoup(data,'html.parser')\n",
    "    for value in soup.findAll('div',{'class':'ooJZfD _2oZ8XT col-8-12'}):\n",
    "        def scrp_name():\n",
    "            naming='Null'\n",
    "            for val in value.findAll('div',{'class':'_29OxBi'}):\n",
    "            \n",
    "                for tx in val.findAll('span',{'class':'_35KyD6'}):\n",
    "                    naming+=tx.text\n",
    "            return naming\n",
    "        name.append(scrp_name())\n",
    "        \n",
    "        def scrap():\n",
    "            count=''\n",
    "            rating='Null'\n",
    "            for val in value.findAll('div',{'class':'a7AaCC'}):\n",
    "                for rt in val.findAll('div',{'class':'hGSR34 bqXGTW'}):\n",
    "                    rating=rt.text\n",
    "                for ct in val.findAll('span',{'class':'_38sUEc'}):\n",
    "                    count+=ct.text\n",
    "            return rating,count\n",
    "        rating.append(scrap()[0])\n",
    "        count.append(scrap()[1])\n",
    "        \n",
    "        def scrap_price():\n",
    "            pricess='Null'\n",
    "            for val in value.findAll('div',{'class':'_3iZgFn _9Z7kX3'}):\n",
    "                for vl in val.findAll('div',{'class':'_1vC4OE _3qQ9m1'}):\n",
    "                    pricess+=vl.text\n",
    "            return pricess\n",
    "        price.append(scrap_price())\n",
    "        def ss():\n",
    "            mist='https://www.flipkart.com'\n",
    "            for val in value.findAll('div',{'class':'col _39LH-M _3cycCZ'}):       \n",
    "                for tx in val.findAll('a'):\n",
    "                    mist+=tx.get('href')\n",
    "                    temp.append(mist)\n",
    "                    mist='https://www.flipkart.com'\n",
    "            return mist,temp\n",
    "        a,b=ss()\n",
    "        if b!=[]:\n",
    "                links.append(temp[-1])\n",
    "        else:\n",
    "                links.append(a)\n",
    "        temp=[]\n",
    "    sneaker4=pd.DataFrame({'Name':name,'Price':price,'Links':links,'Rating':rating,'Count':count})\n",
    "    sneaker4.to_csv(r'E:\\sneaker4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(name))\n",
    "print(len(price))\n",
    "print(len(rating))\n",
    "print(len(count))\n",
    "print(len(links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.Links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.flipkart.com/swiggy-combo-pack-3-casual-sneakers-men/p/itm65144c640a90d?pid=SHOFMZA8UB9ZWX42&lid=LSTSHOFMZA8UB9ZWX42OECFBK&marketplace=FLIPKART&srno=b_1_1&otracker=browse&fm=organic&iid=en_YsiU%2B0zqgl3usyMDIjBW7FRDh4izqgCNHSTqGIW2izLbjKLRU%2BPa6het4qWq0p8v2MhpKgltnhGKOtEYl17xcw%3D%3D&ssid=p30amoq1a80000001597728340114'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.Links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.flipkart.com/swiggy-combo-pack-3-casual-sneakers-men/p/itm65144c640a90d?pid=SHOFMZA8UB9ZWX42&lid=LSTSHOFMZA8UB9ZWX42OECFBK&marketplace=FLIPKART&srno=b_1_1&otracker=browse&fm=organic&iid=en_c4H1HkzJPGqcINwa0wnt%2F22v3fxaqIN7%2Bxkc%2BN93QvWByjMYLXKLAsTTGJjtkJ1I2MhpKgltnhGKOtEYl17xcw%3D%3D&ssid=i22qfb5scg0000001597728505112'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.Links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
